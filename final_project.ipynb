{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project of the Enron Incident\n",
    "\n",
    "This is a notebook of the final project for the Udacity lesson - Mechine Learning - which propose on detecting who is the person of interest(POI) in the Enron Incident by using the Enron email dataset. We'll go through four part of the Mechine Learning progress: Dataset/Question -> Features -> Algorithms -> Evaluation and figure out the POI by our Mechine Learning code. \n",
    "\n",
    "\n",
    "Between each part of coding, I'll answer some questions such as why I choose these feature as the new dataset, and something special during my coding. Finally, at the end of this notebook, I'll answer all the Udacity's question again to meet the requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\dand\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Show the Characteristics of the dataset\n",
    "\n",
    "We firstly show some characteristics of this dataset to have a general understanding of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_poi(dataset):\n",
    "    num_poi = 0\n",
    "    for name in data_dict:\n",
    "        if data_dict[name][\"poi\"] == 1:\n",
    "             num_poi += 1\n",
    "    return num_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data point:  146\n",
      "Number of features:  20\n",
      "Number of poi:  18\n",
      "Number of non-poi:  128\n"
     ]
    }
   ],
   "source": [
    "# Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n",
    "num_poi = count_poi(data_dict)\n",
    "\n",
    "print \"Number of data point: \", len(data_dict)\n",
    "print \"Number of features: \", len(data_dict[data_dict.keys()[0]])-1\n",
    "print \"Number of poi: \", num_poi\n",
    "print \"Number of non-poi: \", len(data_dict) - num_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Remove the outliers\n",
    "\n",
    "Reading the given pdf called *enron61702insiderpay* we know that we have three outliers in ourdataset: 'TOTAL', 'THE TRAVEL AGENCY IN THE PARK’ and 'LOCKHART EUGENE E'. This is because 'TOTAL', ‘TRAVEL AGENCY IN THE PARK’ are not real people, and 'LOCKHART EUGENE E' has no non NaN values. so we need to remove these keys in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data point:  143\n",
      "Number of features:  20\n",
      "Number of poi:  18\n",
      "Number of non-poi:  125\n"
     ]
    }
   ],
   "source": [
    "data_dict.pop(\"TOTAL\")\n",
    "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "data_dict.pop(\"LOCKHART EUGENE E\")\n",
    "\n",
    "num_poi = count_poi(data_dict)\n",
    "\n",
    "print \"Number of data point: \", len(data_dict)\n",
    "print \"Number of features: \", len(data_dict[data_dict.keys()[0]])-1\n",
    "print \"Number of poi: \", num_poi\n",
    "print \"Number of non-poi: \", len(data_dict) - num_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Choose our feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps, we use SelectKBest to select the best 6 features in the whole features_list(except poi, because poi is a label not feature) that most fit to our dataset. Firstly, we choose all the features of our data as the features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features_list is a list of strings, each of which is a feature name.\n",
    "# The first feature must be \"poi\"\n",
    "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', \n",
    "                 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', \n",
    "                 'other', 'long_term_incentive', 'restricted_stock', 'director_fees',\n",
    "                 'to_messages', 'from_poi_to_this_person', 'from_messages',\n",
    "                 'from_this_person_to_poi', 'shared_receipt_with_poi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we calculate the *NaN* in our dataset for each feature. Suppose that if one feature that half of the poeple are missing its value, this feature will be helpless. So we note it down as awful feature and remove them in the features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deferral_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'long_term_incentive', 'director_fees']\n"
     ]
    }
   ],
   "source": [
    "# detect the awful features\n",
    "awful_feature = []\n",
    "for feature in features_list:\n",
    "    count = 0\n",
    "    for name in data_dict:\n",
    "        if data_dict[name][feature] == 'NaN':\n",
    "            count += 1\n",
    "    if count >= 60:\n",
    "        awful_feature.append(feature)\n",
    "    \n",
    "print awful_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the awful features\n",
    "for item in awful_feature:\n",
    "    features_list.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we need to load in our data from the data_dict, so we call the featureFormat function to help us do this job. To save data easily later, we add a name id list to the dataset in the last column, so that we can change the dataset into dict later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort_keys = False to make the name of the poeple to be the original order\n",
    "data = featureFormat(data_dict, features_list, sort_keys = False, remove_all_zeroes = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# delete the all_zero_people\n",
    "name_list = data_dict.keys()\n",
    "name_id = [i for i in range(len(name_list))]\n",
    "\n",
    "# delete the all_zero_data\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code shown below, we import SelectKBest function from sklearn to help us choose the top 6 features from 12 features remained to do our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# define a selector\n",
    "selector = SelectKBest(f_classif, k=6)\n",
    "\n",
    "# transform the features\n",
    "transfrom_features = selector.fit(features, labels).transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the selectKBset function, we print the selector score by print selector.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores of the selector:\n",
      "[ 18.28968404   8.77277773  24.18144555   6.09417331  24.81507973\n",
      "   4.18747751   9.13378204   1.64634113   5.24344971   0.16970095\n",
      "   2.38261211   8.58942073]\n"
     ]
    }
   ],
   "source": [
    "print \"scores of the selector:\\n\", selector.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, We return the index of the selector to see which feature we select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole feature list: \n",
      "['salary', 'total_payments', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'restricted_stock', 'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
      "\n",
      "\n",
      "\n",
      "Index choosed from the feature list: \n",
      "[ 0  1  2  4  6 11]\n",
      "\n",
      "\n",
      "\n",
      "Choosen features' name: \n",
      "['salary' 'total_payments' 'total_stock_value' 'exercised_stock_options'\n",
      " 'restricted_stock' 'shared_receipt_with_poi']\n"
     ]
    }
   ],
   "source": [
    "index = selector.get_support(True)\n",
    "new_features_list = np.array(features_list[1:])[index]\n",
    "print \"The whole feature list: \\n\", features_list[1:]\n",
    "print \"\\n\\n\"\n",
    "print \"Index choosed from the feature list: \\n\", index\n",
    "print \"\\n\\n\"\n",
    "print \"Choosen features' name: \\n\", new_features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Create new features\n",
    "\n",
    "In my opinions, we can create two features call ratio_message_to_poi and ratio_message_from_poi which may hlep the model predict well. These two features can be computed as follow:\n",
    "\n",
    "> ratio_message_to_poi = from_this_person_to_poi / to_messages\n",
    "\n",
    "> ratio_message_from_poi = from_poi_to_this_person / from_messages\n",
    "\n",
    "We will test the influence of these new features in the later session, which compare the dataset with or without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio_message_to_poi = []\n",
    "ratio_message_from_poi = []\n",
    "for i in range(len(data)):\n",
    "    if data[i][8] == 0 or data[i][9] == 0:\n",
    "        ratio_message_to_poi.append(0)\n",
    "    else:\n",
    "        ratio_message_to_poi.append(data[i][9]/data[i][8])\n",
    "        \n",
    "    if data[i][11] == 0 or data[i][10] == 0:\n",
    "        ratio_message_from_poi.append(0)\n",
    "    else:\n",
    "        ratio_message_from_poi.append(data[i][11]/data[i][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add the name id, ratio_message_to_poi, ratio_message_from_poi to the dataset\n",
    "data = np.c_[data, ratio_message_to_poi, ratio_message_from_poi, name_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Scaling\n",
    "\n",
    "Uing the scale function to do Z-score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "data = np.array(data)\n",
    "data[:, 1:-1] = scale(data[:, 1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Store the dataset as new_data_dict\n",
    "\n",
    "we store the dataset after cleaning it above, and this help us easily to call the function in the tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store the data as the original data_dict\n",
    "new_data_dict = {}\n",
    "for people in range(len(data)):\n",
    "    name_id = data[people][-1]\n",
    "    new_data_dict[name_list[int(name_id)]] = {}\n",
    "    new_data_dict[name_list[int(name_id)]][\"poi\"] = data[people][0]\n",
    "    for feature in index:\n",
    "        new_data_dict[name_list[int(name_id)]][features_list[feature+1]] = data[people][feature+1]\n",
    "        \n",
    "    # add the new features value to the dict\n",
    "    new_data_dict[name_list[int(name_id)]][\"ratio_message_to_poi\"] = data[people][-3]\n",
    "    new_data_dict[name_list[int(name_id)]][\"ratio_message_from_poi\"] = data[people][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_features_list = new_features_list.tolist()\n",
    "# add the poi feature to the first of the list\n",
    "new_features_list.insert(0, \"poi\")\n",
    "# add the ratio_message_to_poi, ratio_message_from_poi to the features_list\n",
    "new_features_list.append(\"ratio_message_to_poi\")\n",
    "new_features_list.append(\"ratio_message_from_poi\")\n",
    "\n",
    "data = featureFormat(new_data_dict, new_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Try a varity of classifiers\n",
    "\n",
    "In this session, we'll try a varity of classifiers and choose the best classifiers as our final model. What's more, we'll compare the predicion that when we use the new features created by us before with not using them to show how well these new features done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Validation:\n",
    "\n",
    "For the following bloks, we'll measure how well the new features done to the model. Firstly we'll show the features_list of these two list and  see their different. Then we'll use three mechine learning technic to do the prediction: SVM, DT, NB\n",
    "\n",
    "We 'll call the test_classifier() function to help us measure how well the model predict, this function use StratifiedShuffleSplit function as its' cross_validation function which is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class. \n",
    "\n",
    "The importance of the validation is to prevent our model being overfitting(predict well on the training set but wrose on the validation set), which make sure the reliability of the algorithm we build \n",
    "\n",
    "Finally, the test_classifier() will print Accuracy, Precision, Recall, F1 as the evaluation index, which we'll discuss in the summary session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feautres with the tow new create features:\n",
      "['salary', 'total_payments', 'total_stock_value', 'exercised_stock_options', 'restricted_stock', 'shared_receipt_with_poi', 'ratio_message_to_poi', 'ratio_message_from_poi']\n"
     ]
    }
   ],
   "source": [
    "print \"feautres with the tow new create features:\\n\", new_features_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feautres without the tow new create features:\n",
      "['salary', 'total_payments', 'total_stock_value', 'exercised_stock_options', 'restricted_stock', 'shared_receipt_with_poi']\n"
     ]
    }
   ],
   "source": [
    "old_features_list = new_features_list[:-2]\n",
    "print \"feautres without the tow new create features:\\n\", old_features_list[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the begining of this part, we'll use the  GridSearchCV() function to judge the parameter of the model to show how import parameter tuning is. We'll take the DT model into our consideration and use the new_features_list as our feauters used to make prediction. The algorithm finally give the best choice of paramters that can make the algorithm optimize (criterion:gini, min_samples_leaf:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid={'criterion': ('gini', 'entropy'), 'min_samples_leaf': [1, 3]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring='f1', verbose=0)\n",
      "\tAccuracy: 0.75191\tPrecision: 0.30393\tRecall: 0.28250\tF1: 0.29282\tF2: 0.28654\n",
      "\tTotal predictions: 11000\tTrue positives:  565\tFalse positives: 1294\tFalse negatives: 1435\tTrue negatives: 7706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {'criterion':('gini', 'entropy'), 'min_samples_leaf': [1, 3]}\n",
    "svr = tree.DecisionTreeClassifier()\n",
    "clf = GridSearchCV(svr, parameters, scoring='f1')\n",
    "\n",
    "test_classifier(clf, new_data_dict, new_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use different model to fit our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "\n",
    "This model seems nearly predict all the testset into all people are not poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='sigmoid',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.81236\tPrecision: 0.04286\tRecall: 0.00150\tF1: 0.00290\tF2: 0.00186\n",
      "\tTotal predictions: 11000\tTrue positives:    3\tFalse positives:   67\tFalse negatives: 1997\tTrue negatives: 8933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(svm.SVC(kernel = \"sigmoid\"), new_data_dict, new_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "\n",
    "This model also meet the requirement of the project with precision and recall are more than 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.75327\tPrecision: 0.32703\tRecall: 0.33750\tF1: 0.33219\tF2: 0.33535\n",
      "\tTotal predictions: 11000\tTrue positives:  675\tFalse positives: 1389\tFalse negatives: 1325\tTrue negatives: 7611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(tree.DecisionTreeClassifier(), new_data_dict, new_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNB\n",
    "\n",
    "This model also meet the requirement of the project, and have a highest accuracy and precision meanwhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.80036\tPrecision: 0.39135\tRecall: 0.17650\tF1: 0.24328\tF2: 0.19827\n",
      "\tTotal predictions: 11000\tTrue positives:  353\tFalse positives:  549\tFalse negatives: 1647\tTrue negatives: 8451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(GaussianNB(), new_data_dict, new_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction without new create features:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='sigmoid',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.79880\tPrecision: 0.28571\tRecall: 0.00400\tF1: 0.00789\tF2: 0.00498\n",
      "\tTotal predictions: 10000\tTrue positives:    8\tFalse positives:   20\tFalse negatives: 1992\tTrue negatives: 7980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(svm.SVC(kernel = \"sigmoid\"), new_data_dict, old_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.75000\tPrecision: 0.36249\tRecall: 0.32950\tF1: 0.34521\tF2: 0.33561\n",
      "\tTotal predictions: 10000\tTrue positives:  659\tFalse positives: 1159\tFalse negatives: 1341\tTrue negatives: 6841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(tree.DecisionTreeClassifier(), new_data_dict, old_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.78570\tPrecision: 0.41248\tRecall: 0.16850\tF1: 0.23926\tF2: 0.19111\n",
      "\tTotal predictions: 10000\tTrue positives:  337\tFalse positives:  480\tFalse negatives: 1663\tTrue negatives: 7520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(GaussianNB(), new_data_dict, old_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shown above point that when we use the new create features to make prediciton, we can get a higer accuracy but a lower presicion and other evaluation index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Dump your classifier, dataset, and features_list \n",
    "You do not need to change anything below, but make sure\n",
    "that the version of poi_id.py that you submit can be run on its own and\n",
    "generates the necessary .pkl files for validating your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump_classifier_and_data(tree.DecisionTreeClassifier(), new_data_dict, old_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, firstly, I went through all the data in our dataset and dropout the 'bad features' which got lots of NaN value, then i use SelectKBest function to select the top 6 value atomatically to be our reliable feature list.\n",
    "\n",
    "Secondly, by using matplotlib module, I show all 6 features value by plotting two features each time on the figure to show if there are any outliers in our dataset. After finding these outliers, I sort the dataset by the given feature, and remove these point.\n",
    "\n",
    "Thridly, After doing all the movements mentioned above, I use a scale function to our dataset so that our algorithm can converge faster, which means we can save lots of time in the training option.\n",
    "\n",
    "Finally, we save the dataset as new_data_dict which allow other poeple to run easily.\n",
    "\n",
    "During the local test, I choose three mechine learning technic: NB, SVM, DT, and after runing the code with different parameter, I choose the DT model to be the finall code because it have a better accuarcy and a higher precision rate which meet the requirement meanwhile. \n",
    "\n",
    "In testing our model, I use split my dataset into training set and validation set which in order to estimate how well your model has been trained  and to estimate model properties \n",
    "\n",
    "And I use four evaluation index to show how well the model can be. \n",
    "\n",
    "The accuracy means how many right predicitons your model are made divide how may times your model have to predict. \n",
    "\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives(which means the real label is poi and the predict label is also poi) and fp the number of false positives(which means the real label is non-poi and the predict label is poi)\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives (which means the real label is poi and the predict label is non-poi)\n",
    "\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0\n",
    "\n",
    "> F1 = 2 \\* (precision \\* recall) / (precision + recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
